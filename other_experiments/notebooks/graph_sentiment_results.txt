Twitter sentiment dataset:
# on test data, ~balanced (?)
	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=False, output_number=False, calculate_overall_score=1, threshold=0.05, max_nodes=0
	nlp = spacy.load('en_core_web_sm')
	nlp.add_pipe("merge_noun_chunks")
	for i in range(test_tse_data.shape[0]):
		if i%200==0:
			print(i)
	text_to_check=test_tse_data['text'][i]
	test_tse_data['predicted_sentiment'][i]=graph_sentiment_analysis(text_to_check, calculate_overall_score=1, nlp=nlp)

	- accuracy, ba_tse: (0.6069609507640068, 0.6016598994840155)


	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=True, output_number=False, calculate_overall_score=1, threshold=0.05, max_nodes=0
	- accuracy_compound, ba_tse_compound: (0.6078098471986417, 0.5959738357743798)

	#here when I tried to delete non english, the library did not perform well (probably because the texts were too short to properly detect language) and the performance did not change
	

	#model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
	tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
	- acc, ba: (0.5325410299943407, 0.5755031610879299)

	#model_name="textattack/roberta-base-SST-2"
	model = AutoModelForSequenceClassification.from_pretrained(model_name)
	tokenizer = AutoTokenizer.from_pretrained(model_name)
	input_ids = tokenizer.encode(list_of_contents[i], return_tensors="pt", max_length=512, truncation=True)
	
	- acc, ba: (0.5181097906055461, 0.580623063850625)


News sentiment dataset: 
# about 750 positive, 100 negative
# comments: there are also records not in english, so in one of the versions I delete them with `langdetect` library
	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=False, output_number=False, calculate_overall_score=1, threshold=0.0, max_nodes=0
	- acc, balanced_accuracy: (0.7570754716981132, 0.7626737967914439)

	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=True, output_number=False, calculate_overall_score=1, threshold=0.0, max_nodes=0
	- acc, ba: (0.7382075471698113, 0.7649732620320856)

	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=True, output_number=False, calculate_overall_score=1, threshold=0.0, max_nodes=0
	# non english deleted
	- acc, ba: (0.7458233890214797, 0.7801174228195389)

	
	#model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
	tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
	- acc, ba: (0.5577830188679245, 0.5284224598930481)

	#model_name="textattack/roberta-base-SST-2"
	model = AutoModelForSequenceClassification.from_pretrained(model_name)
	tokenizer = AutoTokenizer.from_pretrained(model_name)
	input_ids = tokenizer.encode(list_of_contents[i], return_tensors="pt", max_length=512, truncation=True)
	- acc, ba: (0.7900943396226415, 0.7770588235294118)




