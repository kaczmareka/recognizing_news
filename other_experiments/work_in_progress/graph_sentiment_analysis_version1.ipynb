{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJdpAjvi1kDA",
        "outputId": "5bd3bcdd-7f05-45df-b4c5-28959199ce06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vader scores calculation: https://github.dev/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py\n",
        "- compound: float(sum(sentiments_of_words)) +/- coefficient connected to number of \"!\" and \"?\" in the sentence; later normalized\n",
        "- pos: pos_sum +/- coefficient connected to number of \"!\" and \"?\", divided by total sentiment and abs\n",
        "- neg: neg_sum +/- coefficient connected to number of \"!\" and \"?\", divided by total sentiment(pos+abs(neg)+neutral) and abs\n",
        "- neu: neu_count, divided by total sentiment(pos+abs(neg)+neutral) and abs\n"
      ],
      "metadata": {
        "id": "Ch_NFMaEdC__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis - graph based\n",
        "\n",
        "Sentiment analysis - graph based - general idea:\n",
        "\n",
        "For each text in dataset to get its sentiment:\n",
        "1. Get nouns, verbs, adjectives and NERs (e.g. with nltk library) - they will be the nodes of the graph. Only single words, no bigrams.\n",
        "2. The weights of edges between nodes will be created based on the distance in the original text between two given words. The weight will be the sum of inverses of distances between them in the whole text.\n",
        "3. For each word in node the sentiment will be calculated (e.g. with VADER model). The final outcome will be -1 for negative, 0 for neutral and 1 for positive.\n",
        "4. The sentiment will later be \"weighted\". This means that for each node the sentiment from VADER will be multiplied by scaled_sum_of_weights_of_edges_to_this_node (mean of sum of weights * number of occurencies of the word in original text).\n",
        "5. The sentiment of the whole text will be the normalized sum of weights for all nodes. There will be a certain threshold, which scores will be treated as neutral. In general, if sum_of_sentiment > 0 than positive, < 0 negative, ~0 neutral.\n",
        "\n",
        "\n",
        "\n",
        "More details:\n",
        "- there will be a possibility to choose\n",
        "\t\t- maximum number of nodes in the graph - selected will be topk words with most occurencies in the text\n",
        "\t\t- max_distance between words to add the inverse to edge weight\n",
        "\t\t- NER_list to provide NERs as the important ones to calculate and return the sentiment score for them instead of for the whole text\n",
        "\t\t- if_calculate_overall_score -> to say if user wants to get the score of the whole text or just for given NERs\n",
        "- maybe just available dict with words and their sentiment"
      ],
      "metadata": {
        "id": "VWmJFwOu_Dw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideas used:\n",
        "- for NER https://spacy.io/api/entityrecognizer\n",
        "\n",
        "- https://www.nltk.org/book/ch05.html\n",
        "\n",
        "- \"NLTK offers flexible algorithms for tasks like tokenization and part-of-speech tagging, while spaCy is renowned for its speed and performance, ideal for efficient NLP solutions.\"\n",
        "\n",
        "- https://spacy.io/usage/linguistic-features"
      ],
      "metadata": {
        "id": "k7c8HpGwE_n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "import spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_3G_lWCAFsH",
        "outputId": "bca89454-ec5e-4721-fed8-b1c9260e178a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_ents(doc):\n",
        "  list_of_ents=[]\n",
        "  if doc.ents:\n",
        "    for ent in doc.ents:\n",
        "      list_of_ents.append(ent.text)\n",
        "  return list_of_ents"
      ],
      "metadata": {
        "id": "jXV0G_ciCn0-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_for_nodes(text, list_ners=[], lemmatization=False):\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  nlp.add_pipe(\"merge_noun_chunks\")\n",
        "  doc1 = nlp(text)\n",
        "  # find ners\n",
        "  if type(list_ners)==list and len(list_ners)>0:\n",
        "    ners=list_ners\n",
        "  else:\n",
        "    ners = find_ents(doc1) #maybe TO DO: find better way to get NER (what was Actaware idea for that? they are happy with it, so...)\n",
        "    print(ners)\n",
        "  # if I put lemmatization before ners, they do not catch everything, e.g. \"NY\"\n",
        "  if lemmatization:\n",
        "    #a co z podmiankami w tekście, jeśli on zwróci all w których jest \"be\" a w tekście są \"is\"?\n",
        "    #TO DO: one of the experiments: does lemmatization change accuracy? and what is the influence on the performance?\n",
        "    lemmatized_tokens=[token.lemma_ for token in doc1]\n",
        "    text = ' '.join(lemmatized_tokens)\n",
        "    doc1=nlp(text)\n",
        "    # print(text)\n",
        "  tags=['JJ', 'JJR', 'JJS', 'NN', 'NNP', 'NNS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "  nouns_verbs_etc = [token.text for token in doc1 if token.tag_ in tags]\n",
        "  text2=\" \".join([str(word).replace(\" \", \"_\") for word in list(doc1)])\n",
        "  all_nodes=[str(word).replace(\" \", \"_\") for word in list(set(ners+nouns_verbs_etc))]\n",
        "  return all_nodes,text2, ners"
      ],
      "metadata": {
        "id": "dKvMNtT7_OBw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all, text3, ners = get_words_for_nodes(\"what is super cool in NewYork and Abu Dhabi, this is NewYork\", lemmatization=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bKIqBDAzzgN",
        "outputId": "005ab2fd-939f-44d0-8701-1a3c750998a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NewYork', 'Abu Dhabi', 'NewYork']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7j0uNilEaZg",
        "outputId": "a3198928-5043-4c52-e0c7-b3625d17e077"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be', 'NewYork', 'super', 'cool', 'Abu_Dhabi']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JS2EOfBdEZzW",
        "outputId": "2f876534-96b8-4be4-d5c1-1003dffd7ae4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what be super cool in NewYork and Abu_Dhabi , this be NewYork'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Links:\n",
        "- https://spacy.io/universe/project/video-spacys-ner-model\n",
        "- https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
      ],
      "metadata": {
        "id": "EHe2N7eNBtev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str1=\"what is good in New York?\"\n",
        "str2=\"New York\"\n",
        "\n",
        "str2 in str1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HJbUXAnC_zT",
        "outputId": "7874189f-b42f-4431-b6a2-22704725609d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## weights of edges"
      ],
      "metadata": {
        "id": "EiRxRvYiGe0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "yzpzI39AJb32"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights_of_edges(text, words, max_distance=20, ner_list=[]):\n",
        "  # if ner_list not empty, we should calculate the distances only between ners and other words\n",
        "  # and do not between other words and other words\n",
        "\n",
        "  list_from_text = text.split()\n",
        "  weight_matrix=np.zeros((len(words), len(words)))\n",
        "  occurences_list=np.zeros(len(words))\n",
        "  for i, word1 in enumerate(list_from_text):\n",
        "    j=i+1\n",
        "    try:\n",
        "      index1=words.index(word1)\n",
        "      occurences_list[index1]+=1\n",
        "    except ValueError:\n",
        "      pass\n",
        "    while j<=i+max_distance and j<len(list_from_text):\n",
        "      word2=list_from_text[j]\n",
        "      if len(ner_list) != 0 and (word1 in ner_list or word2 in ner_list):\n",
        "        try:\n",
        "          index1=words.index(word1)\n",
        "          index2=words.index(word2)\n",
        "          distance=j-i\n",
        "          if distance!=0:\n",
        "            inv_distance=1/distance\n",
        "            weight_matrix[index1][index2]+=inv_distance\n",
        "        except ValueError:\n",
        "          pass\n",
        "      elif len(ner_list)==0:\n",
        "        try:\n",
        "          index1=words.index(word1)\n",
        "          index2=words.index(word2)\n",
        "          distance=j-i\n",
        "          if distance!=0:\n",
        "            inv_distance=1/distance\n",
        "            weight_matrix[index1][index2]+=inv_distance\n",
        "        except ValueError:\n",
        "          pass\n",
        "      else:\n",
        "        pass\n",
        "      j+=1\n",
        "  upper_right_ones=np.triu(np.ones(len(words)))\n",
        "  return (weight_matrix+weight_matrix.T)*upper_right_ones, occurences_list"
      ],
      "metadata": {
        "id": "LnHlYAKKGgja"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3xjqYc5uKq5B",
        "outputId": "ac52f293-e344-4e35-c398-906007c1a536"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what be super cool in NewYork and Abu_Dhabi , this be NewYork'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4sPeG4bKrVk",
        "outputId": "57f839b4-b8fd-4f40-c95c-096abac5acfa"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be', 'NewYork', 'super', 'cool', 'Abu_Dhabi']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight_matrix, occ_list=get_weights_of_edges(text3, all, max_distance=20, ner_list=[])"
      ],
      "metadata": {
        "id": "m7YoQnaZKs2h"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occ_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV7Zmv8QZg3T",
        "outputId": "04517e99-4c4f-41fb-9f61-36d878a85592"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2., 2., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjN9P6hCapF5",
        "outputId": "daf0830c-c70e-4cfa-ad40-e56b7e7cf32a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22222222, 1.55      , 1.125     , 0.64285714, 0.5       ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sentiment for each node\n",
        "\n",
        "'compound' - The normalized compound score which calculates the sum of all lexicon ratings and takes values from -1 to 1"
      ],
      "metadata": {
        "id": "3Wi4AK6sdp34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdL2QN43dwmM",
        "outputId": "9780704e-199f-474b-95e6-159ef3c8b53f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentiment_for_nodes(text, compound=False):\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  # Loop through the words/ bigrams from 1. and get the sentiment scores for each one\n",
        "  sentiment_scores=np.zeros(len(text))\n",
        "  for i, text in enumerate(text):\n",
        "    scores=analyzer.polarity_scores(text)\n",
        "    if compound:\n",
        "      sentiment_scores[i]=scores['compound']\n",
        "    else:\n",
        "      if scores['neg']==1.0:\n",
        "        sentiment_scores[i]=-1\n",
        "      elif scores['pos']==1.0:\n",
        "        sentiment_scores[i]=1\n",
        "    print(text)\n",
        "    print(scores)\n",
        "  return sentiment_scores"
      ],
      "metadata": {
        "id": "6VXXmt_B6qPy"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_sentiment_for_nodes(all, compound=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYfSzgXkfQ4F",
        "outputId": "57a78535-b476-4b73-d741-0329115739ef"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "be\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "NewYork\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "super\n",
            "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5994}\n",
            "cool\n",
            "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.3182}\n",
            "Abu_Dhabi\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_scores=calculate_sentiment_for_nodes(all, compound=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAkc3RwBfVFT",
        "outputId": "3b8bb8df-a5de-43e7-9bd2-8d54fcfb55a5"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "be\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "NewYork\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "super\n",
            "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5994}\n",
            "cool\n",
            "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.3182}\n",
            "Abu_Dhabi\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmAvcJoad96F",
        "outputId": "fbebf16a-9e71-4b11-9e68-073eee809c47"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores['neg']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejhK1c0Xepit",
        "outputId": "451d96da-c775-4f57-f075-06c215fed4d7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# weighting of the sentiment"
      ],
      "metadata": {
        "id": "kPRPzn1Bfgxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH5-VIh_erCu",
        "outputId": "32b16f93-556f-49d8-abc3-f3e8566ab73b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22222222, 1.55      , 1.125     , 0.64285714, 0.5       ],\n",
              "       [0.        , 0.33333333, 0.44444444, 0.625     , 0.75      ],\n",
              "       [0.        , 0.        , 0.        , 1.        , 0.2       ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.25      ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJkTIepIfpyZ",
        "outputId": "3c8f084f-8494-4ca8-d132-c72d07b2a66e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.    , 0.    , 0.5994, 0.3182, 0.    ])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "occ_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OUi_ddmfq_l",
        "outputId": "97975f14-f121-4766-bb22-6f6460463048"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2., 2., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_sentiment(weight_matrix, occ_list, sentiment_scores):\n",
        "  sum_columns_weights=np.sum(weight_matrix, axis=0)\n",
        "  count_nonzero_weights=np.count_nonzero(weight_matrix, axis=0)\n",
        "  mean_columns_weights=np.sum(weight_matrix, axis=0)/np.count_nonzero(weight_matrix, axis=0)\n",
        "  total_weight=(np.sum(weight_matrix, axis=0)/np.count_nonzero(weight_matrix, axis=0))*occ_list\n",
        "  return total_weight*sentiment_scores"
      ],
      "metadata": {
        "id": "M-uPFw8bglqE"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_sentiment=weighted_sentiment(weight_matrix, occ_list, sentiment_scores)"
      ],
      "metadata": {
        "id": "0RoLKhtgg2c0"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sentiment of the whole text\n",
        "\n",
        "normalized?"
      ],
      "metadata": {
        "id": "B8rNsENihdp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(weighted_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6ILtZEehqaN",
        "outputId": "f2ef68f2-fb6f-45c0-e9d1-19d90528a61a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7109065476190476"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentiment_of_text(weighted_sentiment, threshold=0.05):\n",
        "  sum_sentiment=np.sum(weighted_sentiment)\n",
        "  if sum_sentiment>threshold:\n",
        "    return \"positive\"\n",
        "  elif sum_sentiment<-threshold:\n",
        "    return \"negative\"\n",
        "  else:\n",
        "    return \"neutral\""
      ],
      "metadata": {
        "id": "UDFFM9Ohg8M9"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_sentiment_of_text(weighted_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PMvf6QtCiBhe",
        "outputId": "41286fe6-ef37-45b9-b766-aacbbcde9840"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_sentiment_of_text([-1,2,-4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gfHtEidkiRuU",
        "outputId": "9b517307-e875-47a2-ea59-a6815d7b9813"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_sentiment_of_text([0.5, -0.45])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "D2fY3u7SiQB0",
        "outputId": "51cd5035-bf68-4058-fd19-6ff0c10df783"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neutral'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    }
  ]
}